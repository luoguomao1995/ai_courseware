{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ee8ce2-4b03-4b57-b85d-b4fb6db26570",
   "metadata": {},
   "source": [
    "# 💡 这节课会带给你"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfbc193-f8fc-4e18-85e7-45d3e8a6383a",
   "metadata": {},
   "source": [
    "1. 系统性维护、测试、监控一个 LLM 应用\n",
    "2. 学习使用主流的工具完成上述工作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe2fb93-5991-48e1-898b-a48ebfda9481",
   "metadata": {},
   "source": [
    "## 维护一个生产级的 LLM 应用，我们需要做什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb536c0-3997-457a-b360-62fbbc910454",
   "metadata": {},
   "source": [
    "1. 调试 Prompt\n",
    "2. Prompt 版本管理\n",
    "3. 测试/验证系统的相关指标\n",
    "4. 数据集管理\n",
    "5. 各种指标监控与统计：访问量、响应时长、Token费等等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5579d2ef-95ed-4e05-a025-940434a88100",
   "metadata": {},
   "source": [
    "### 针对以上需求，我们介绍三个生产级 LLM App 维护平台"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb8637-fd33-438c-bfba-5acc62ff8500",
   "metadata": {},
   "source": [
    "1. **LangSmith**: LangChain 的官方平台，SaaS 服务，非开源；\n",
    "2. **LangFuse**: 开源 + SaaS，LangSmith 平替，可集成 LangChain 也可直接对接 OpenAI API；\n",
    "3. **Prompt Flow**：微软开发，开源 + Azure AI云服务，可集成 Semantic Kernel。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f2552-f3af-4484-91e3-3c2308c05921",
   "metadata": {},
   "source": [
    "## 1、LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f950af-34cd-43f1-a1a2-ed18d3743f1f",
   "metadata": {},
   "source": [
    "平台入口：https://www.langchain.com/langsmith\n",
    "\n",
    "文档地址：https://python.langchain.com/docs/langsmith/walkthrough\n",
    "\n",
    "将你的 LangChain 应用与 LangSmith 链接，需要：\n",
    "\n",
    "1. 注册账号，并申请一个`LANGCHAIN_API_KEY`\n",
    "2. 在环境变量中设置以下值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77875bb-76ca-4737-afa9-5796064e6e10",
   "metadata": {},
   "source": [
    "```shell\n",
    "export LANGCHAIN_TRACING_V2=true\n",
    "export LANGCHAIN_PROJECT=YOUR_PROJECT_NAME #自定义项目名称\n",
    "export LANGCHAIN_ENDPOINT=https://api.smith.langchain.com #LangSmith的服务端点\n",
    "export LANGCHAIN_API_KEY=LANGCHAIN_API_KEY # LangChain API Key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de823435-67b7-4f96-974a-4588fad8a1ef",
   "metadata": {},
   "source": [
    "3. 程序中的调用将自动被记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58e99900-e48c-4817-a1d4-fe3a101c62bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"agi_demo_hello_world\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"ls__a5a4ca7a021342748af8127bb805ba0a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dddaf145-15f2-450e-903c-2de85d6aeb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello 中国!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# 定义语言模型\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# 定义Prompt模板\n",
    "prompt = PromptTemplate.from_template(\"Say hello to {input}!\")\n",
    "\n",
    "# 定义输出解析器\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = (\n",
    "    {\"input\":RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"中国\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59edfa6-2282-48ee-9c97-7af6a2532ddf",
   "metadata": {},
   "source": [
    "<img src=\"langsmith-example.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a31a3eb-9fbf-408c-b2fc-5b5f3cb16268",
   "metadata": {},
   "source": [
    "### 1.1、基本功能\n",
    "\n",
    "1. Traces\n",
    "2. LLM Calls\n",
    "3. Monitor\n",
    "4. Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c59c7c0-9e49-4230-841c-900487b716cb",
   "metadata": {},
   "source": [
    "### 1.2、Dataset & Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ffe88-06b1-4dc0-8937-88b9477bba79",
   "metadata": {},
   "source": [
    "在产品中使用一个 AI 功能，我们首先需要系统性测试它的能力指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daddedd-c3aa-410f-a4e4-b9d3e6a039ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3eea81ac-a705-4447-9ced-228c93a5cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import WikipediaRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Answer user's question according to the context below. \n",
    "Be brief, answer in no more than 20 words.\n",
    "CONTEXT_START\n",
    "{context}\n",
    "CONTEXT_END\n",
    "\n",
    "USER QUESTION:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "# 检索 wikipedia\n",
    "retriever = WikipediaRetriever(top_k_results=3)\n",
    "\n",
    "def chain_constructor(retriever):\n",
    "    # 定义语言模型\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        temperature=0,\n",
    "    )\n",
    "    \n",
    "    # 定义Prompt模板\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        prompt_template\n",
    "    )\n",
    "    \n",
    "    # 定义输出解析器\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    response_generator = (\n",
    "        #{\"context\":retriever,\"input\":RunnablePassthrough()}\n",
    "        prompt \n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"input\") | retriever | (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "            \"input\":  itemgetter(\"input\")\n",
    "        }\n",
    "        | response_generator\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead1e1d-b602-4af9-a249-19131dd48554",
   "metadata": {},
   "source": [
    "第一步，我们需要准备一个数据集，包含输入与预期输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bb67c3b-c62c-446c-98f6-2dd93c866e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "qa_pairs = []\n",
    "with open('example_dataset.jsonl','r',encoding='utf-8') as fp:\n",
    "    for line in fp:\n",
    "        example = json.loads(line.strip())\n",
    "        qa_pairs.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e978a2d-74cd-4b9c-a3cb-18f81b091c67",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "[Errno 400 Client Error: Bad Request for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Dataset with this name already exists.\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langsmith/utils.py:83\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api.smith.langchain.com/datasets",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m Client()\n\u001b[1;32m      5\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwiki_qa_dataset_demo_100\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#数据集名称\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m一个数据集样例，从wiki_qa benchmark中抽取的100条问答对\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#数据集描述\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m qa_pairs:\n\u001b[1;32m     13\u001b[0m     client\u001b[38;5;241m.\u001b[39mcreate_example(\n\u001b[1;32m     14\u001b[0m         inputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]}, outputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]}, dataset_id\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mid\n\u001b[1;32m     15\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langsmith/client.py:1282\u001b[0m, in \u001b[0;36mClient.create_dataset\u001b[0;34m(self, dataset_name, description, data_type)\u001b[0m\n\u001b[1;32m   1272\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ls_schemas\u001b[38;5;241m.\u001b[39mDatasetCreate(\n\u001b[1;32m   1273\u001b[0m     name\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[1;32m   1274\u001b[0m     description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[1;32m   1275\u001b[0m     data_type\u001b[38;5;241m=\u001b[39mdata_type,\n\u001b[1;32m   1276\u001b[0m )\n\u001b[1;32m   1277\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1279\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_headers, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   1280\u001b[0m     data\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mjson(),\n\u001b[1;32m   1281\u001b[0m )\n\u001b[0;32m-> 1282\u001b[0m \u001b[43mls_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status_with_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson(),\n\u001b[1;32m   1285\u001b[0m     _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url,\n\u001b[1;32m   1286\u001b[0m     _tenant_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tenant_id(),\n\u001b[1;32m   1287\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langsmith/utils.py:85\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     83\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHTTPError\u001b[0m: [Errno 400 Client Error: Bad Request for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Dataset with this name already exists.\"}"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = \"wiki_qa_dataset_demo_100\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name, #数据集名称\n",
    "    description=\"一个数据集样例，从wiki_qa benchmark中抽取的100条问答对\", #数据集描述\n",
    ")\n",
    "\n",
    "for example in qa_pairs:\n",
    "    client.create_example(\n",
    "        inputs={\"input\": example['question']}, outputs={\"output\": example['answer']}, dataset_id=dataset.id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f473e5-3873-42d0-b900-f0d9cea94412",
   "metadata": {},
   "source": [
    "第二步，我们定义评估函数，用于数值化的评估模型的实际输出与预期输出之间的差距"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3813b43c-7ecd-46e3-9a2c-9b880bfea413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    # 评估器，可多选\n",
    "    evaluators=[\n",
    "        # 根据答案判断回复是否\"Correct\"\n",
    "        EvaluatorType.QA,\n",
    "    ],\n",
    "    # 可追加自定评估标准\n",
    "    custom_evaluators=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f2e89-0bc6-41df-b5b4-7fa938639795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'AGIClass_LangChain_WikiQA_Project-d37fcdde' at:\n",
      "https://smith.langchain.com/o/97b8262a-9ab9-4b43-afeb-21ea05a90ba7/projects/p/32d10247-25be-49e5-8d83-619e0ea36228?eval=true\n",
      "\n",
      "View all tests for Dataset wiki_qa_dataset_demo_100 at:\n",
      "https://smith.langchain.com/o/97b8262a-9ab9-4b43-afeb-21ea05a90ba7/datasets/e08eb251-fd32-4fe4-93bc-95fcd30c7167\n",
      "[------------------------------------------------->] 99/100"
     ]
    }
   ],
   "source": [
    "from langchain.smith import (\n",
    "    arun_on_dataset,\n",
    "    run_on_dataset,\n",
    ")\n",
    "from uuid import uuid4\n",
    "\n",
    "unique_id = uuid4().hex[0:8]\n",
    "\n",
    "chain = chain_constructor(retriever)\n",
    "\n",
    "chain_results = await arun_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=chain,\n",
    "    evaluation=evaluation_config,\n",
    "    verbose=True,\n",
    "    client=client,\n",
    "    project_name=f\"AGIClass_LangChain_WikiQA_Project-{unique_id}\",\n",
    "    tags=[\n",
    "        \"testing-agiclass-demo\",\n",
    "        \"2023-11-30\",\n",
    "    ],  # 可选，自定义的标识\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5990c519-5615-4a53-9ca9-4e9bd7885ff9",
   "metadata": {},
   "source": [
    "### 1.3、自定义评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "195f62cc-43ae-4120-9008-a2ac456cfd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import StringEvaluator\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import re\n",
    "from typing import Optional, Any\n",
    "\n",
    "class BleuEvaluator(StringEvaluator):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def requires_input(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def requires_reference(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def evaluation_name(self) -> str:\n",
    "        return \"bleu_score\"\n",
    "\n",
    "    def _tokenize(self,sentence):\n",
    "        # 正则表达式定义了要去除的标点符号\n",
    "        return re.sub(r'[^\\w\\s]', '', sentence.lower()).split()\n",
    "    \n",
    "    def _evaluate_strings(\n",
    "        self,\n",
    "        prediction: str,\n",
    "        input: Optional[str] = None,\n",
    "        reference: Optional[str] = None,\n",
    "        **kwargs: Any\n",
    "    ) -> dict:\n",
    "        bleu_score = sentence_bleu(\n",
    "            [self._tokenize(reference)], \n",
    "            self._tokenize(prediction), \n",
    "            smoothing_function=SmoothingFunction().method3\n",
    "        )\n",
    "        return {\"score\": bleu_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc5d4a3b-8cbd-4f86-b079-11e812204b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'AGIClass_LangChain_Custom_Project-2b31e247' at:\n",
      "https://smith.langchain.com/o/97b8262a-9ab9-4b43-afeb-21ea05a90ba7/projects/p/9f795ab0-376c-48c4-b3ce-37cefef756dd?eval=true\n",
      "\n",
      "View all tests for Dataset wiki_qa_dataset_demo_100 at:\n",
      "https://smith.langchain.com/o/97b8262a-9ab9-4b43-afeb-21ea05a90ba7/datasets/e08eb251-fd32-4fe4-93bc-95fcd30c7167\n",
      "[------------------------------------------------->] 100/100"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "from langchain.smith import (\n",
    "    arun_on_dataset,\n",
    "    run_on_dataset,\n",
    ")\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    # 自定义的BLEU SCORE评估器\n",
    "    custom_evaluators=[BleuEvaluator()],\n",
    ")\n",
    "\n",
    "unique_id = uuid4().hex[0:8]\n",
    "chain = chain_constructor(retriever)\n",
    "\n",
    "chain_results = await arun_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=chain,\n",
    "    evaluation=evaluation_config,\n",
    "    verbose=True,\n",
    "    client=client,\n",
    "    project_name=f\"AGIClass_LangChain_Custom_Project-{unique_id}\",\n",
    "    tags=[\n",
    "        \"testing-agiclass-demo\",\n",
    "        \"2023-11\",\n",
    "    ],  # 可选，自定义的标识\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8f7ea-c259-4274-9311-44f8014bf92a",
   "metadata": {},
   "source": [
    "**额外知识**: BLEU Score\n",
    "\n",
    "传统NLP中，对机器翻译或文本生成类模型效果的自动评估的常用方法之一，原理如下：\n",
    "  - 计算输出与参照句之间的 n-gram 准确率（n=1...4）\n",
    "  - 对短输出做惩罚\n",
    "  - 在整个测试集上平均下述值\n",
    "\n",
    "$\\mathrm{BLEU}_4=\\min\\left(1,\\frac{output-length}{reference-length}\\right)\\left(\\prod_{i=1}^4 precision_i\\right)^{\\frac{1}{4}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8858919a-2a93-47b3-a375-b4ee9dff056f",
   "metadata": {},
   "source": [
    "## 2、说说文本生成常用的评估方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883cb589-68a0-4cb0-b31c-505ae347b0e1",
   "metadata": {},
   "source": [
    "假设文本生成问题，我们有或没有参考答案 reference 时，\n",
    "\n",
    "怎么评估模型生成的结果的优劣？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28e3e7-7303-454f-a259-0b8f49b29b13",
   "metadata": {},
   "source": [
    "### 2.1、基于大模型本身做评估\n",
    "\n",
    "https://docs.smith.langchain.com/evaluation/evaluator-implementations\n",
    "\n",
    "1. 正确性（Correctness）：用 LLM 判断给定真实答案的前提下，模型生成的答案是否正确"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26837e8-cc42-4449-9bb1-a0bfe6ee5f5f",
   "metadata": {},
   "source": [
    "```python\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n",
    "You are grading the following question:\n",
    "{query}\n",
    "Here is the real answer:\n",
    "{answer}\n",
    "You are grading the following predicted answer:\n",
    "{result}\n",
    "Respond with CORRECT or INCORRECT:\n",
    "Grade:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE\n",
    ")\n",
    "eval_llm = ChatAnthropic(temperature=0.0)\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        RunEvalConfig.QA(llm=eval_llm, prompt=PROMPT),\n",
    "        RunEvalConfig.ContextQA(llm=eval_llm),\n",
    "        RunEvalConfig.CoTQA(llm=eval_llm),\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970661a2-8d55-4213-b2b2-b701f0b28106",
   "metadata": {},
   "source": [
    "2. 符合标准（Criteria）：无参考答案时，判断输出是否符合特定标准"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58202832-a597-448c-9e3f-8062dbde7f87",
   "metadata": {},
   "source": [
    "```python\n",
    "from langsmith import Client\r\n",
    "from langchain.smith import RunEvalConfig, run_on_dataset\r\n",
    "\r\n",
    "evaluation_config = RunEvalConfig(\r\n",
    "    evaluators=[\r\n",
    "        # You can define an arbitrary criterion as a key: value pair in the criteria dict\r\n",
    "        RunEvalConfig.Criteria({\"creativity\": \"Is this submission creative, imaginative, or novel?\"}),\r\n",
    "        # We provide some simple default criteria like \"conciseness\" you can use as well\r\n",
    "        RunEvalConfig.Criteria(\"conciseness\"),\r\n",
    "    ]\r\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4be210-27fe-4e00-8254-515bb9ef0a18",
   "metadata": {},
   "source": [
    "3. 有帮助（Helpfulness）：根据参考答案判断模型输出是否有帮助"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd0d475-9c08-4bca-a4bb-6f31958d6087",
   "metadata": {},
   "source": [
    "```python\n",
    "from langsmith import Client\n",
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # You can define an arbitrary criterion as a key: value pair in the criteria dict\n",
    "        RunEvalConfig.LabeledCriteria(\n",
    "            {\n",
    "                \"helpfulness\": (\n",
    "                    \"Is this submission helpful to the user,\"\n",
    "                    \" taking into account the correct reference answer?\"\n",
    "                )\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d74fcf4-d84c-4b30-9858-02c5ddd45e7a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>划重点：</b>此类方法，对于用于评估的 LLM 自身能力有要求。需根据具体情况选择使用。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2eec79-5483-4ccb-8ff5-6fcdc8dc148b",
   "metadata": {},
   "source": [
    "### 2.2、一些经典 NLP 的评测方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a49ec-58ff-4d85-8f89-67c932b59b80",
   "metadata": {},
   "source": [
    "1. **编辑距离**：也叫莱文斯坦距离(Levenshtein),是针对二个字符串的差异程度的量化量测，量测方式是看至少需要多少次的处理才能将一个字符串变成另一个字符串。\n",
    "   - 具体计算过程是一个动态规划算法：https://zhuanlan.zhihu.com/p/164599274\n",
    "   - 衡量两个句子的相似度时，可以以词为单位计算\n",
    "2. **BLEU Score**:\n",
    "   - 计算输出与参照句之间的 n-gram 准确率（n=1...4）\n",
    "   - 对短输出做惩罚\n",
    "   - 在整个测试集上平均下述值\n",
    "3. **Rouge Score**:\n",
    "   - Rouge-N：将模型生成的结果和标准结果按 N-gram 拆分后，只计算召回率；\n",
    "   - Rouge-L: 利用了最长公共子序列（Longest Common Sequence），计算：$P=\\frac{LCS(c,r)}{len(c)}$, $R=\\frac{LCS(c,r)}{len(r)}$, $F=\\frac{(1+\\beta^2)PR}{R+\\beta^2P}$\n",
    "   - 函数库：https://pypi.org/project/rouge-score/\n",
    "   - 对比 BLEU 与 ROUGE：\n",
    "     - BLEU 能评估流畅度，但指标偏向于较短的翻译结果（brevity penalty 没有想象中那么强）\n",
    "     - ROUGE 不管流畅度，所以只适合深度学习的生成模型：结果都是流畅的前提下，ROUGE 反应参照句中多少内容被生成的句子包含（召回）\n",
    "5. **METEOR**: 另一个从机器翻译领域借鉴的指标。与 BLEU 相比，METEOR 考虑了更多的因素，如同义词匹配、词干匹配、词序等，因此它通常被认为是一个更全面的评价指标。\n",
    "   - 对语言学和语义词表有依赖，所以对语言依赖强。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02c4c3d-eb17-420e-af3e-23b9e2e89752",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>划重点：</b>此类方法常用于对文本生成模型的自动化评估。实际使用中，我们通常更关注相对变化而不是绝对值（调优过程中指标是不是在变好）。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2880a7-409b-405f-b202-c1e6206603d0",
   "metadata": {},
   "source": [
    "## 3、LangFuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de6111-677f-4599-a342-9501cc756404",
   "metadata": {},
   "source": [
    "功能与 LangSmith 基本重合，开源，支持 LangChain 集成或原生 OpenAI API 集成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681acec-6906-4490-95a8-3717130ce073",
   "metadata": {},
   "source": [
    "<img src=\"langfuse.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c4550-2be4-4989-81fc-40a1539fdb97",
   "metadata": {},
   "source": [
    "1. 通过官方云服务使用：\n",
    "   - 注册: cloud.langfuse.com\n",
    "   - 创建 API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe54f6c-cc98-494f-b5c4-d9c125564d27",
   "metadata": {},
   "source": [
    "```sh\n",
    "LANGFUSE_SECRET_KEY=\"sk-lf-...\"\n",
    "LANGFUSE_PUBLIC_KEY=\"pk-lf-...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049b1e6-2e06-439c-bcf8-c776f6dd5d32",
   "metadata": {},
   "source": [
    "2. 通过 Docker 本地部署"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9fbe43-94fc-4f9f-be9c-190580219940",
   "metadata": {},
   "source": [
    "```sh\n",
    "# Clone repository\n",
    "git clone https://github.com/langfuse/langfuse.git\n",
    "cd langfuse\n",
    " \n",
    "# Run server and db\n",
    "docker compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75383b1-fa32-4456-8c58-9ebdd6cf841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33e0d7-682a-4eb7-97be-52f98bfe0055",
   "metadata": {},
   "source": [
    "### 3.1、替换 OpenAI 客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a616c4-a9e1-434d-bebe-6748fca8d5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langfuse.openai import openai\n",
    "import os\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "  name=\"test-chat\",\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"你是个测试版机器人。\"},\n",
    "      {\"role\": \"user\", \"content\": \"对我说'Hello, World!'\"}],\n",
    "  temperature=0,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6f5e7-2b9f-4c36-ad7e-ad4c88c07ed4",
   "metadata": {},
   "source": [
    "### 3.2、通过 LangChain 的回调集成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c47782b-9b28-4490-907a-0aa54f0b3e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "handler = CallbackHandler(\n",
    "    os.getenv(\"LANGFUSE_PUBLIC_KEY\"), \n",
    "    os.getenv(\"LANGFUSE_SECRET_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee41e249-1151-440b-bba0-334cd024b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "from langchain.chat_models import ErnieBotChat\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "baidu_model = ErnieBotChat()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\"Say hello to {input}!\") \n",
    "])\n",
    "\n",
    "\n",
    "# 定义输出解析器\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = (\n",
    "    {\"input\":RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | baidu_model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2a71703-76d6-450f-9a1c-e0bca46ddde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:langfuse:'model_name'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/langfuse/callback.py\", line 522, in __on_llm_action\n",
      "    model_name = kwargs[\"invocation_params\"][\"model_name\"]\n",
      "                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
      "KeyError: 'model_name'\n",
      "ERROR:langfuse:run not found\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/langfuse/callback.py\", line 593, in on_llm_end\n",
      "    raise Exception(\"run not found\")\n",
      "Exception: run not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'文心 (wenhunjing) 是个什么产品/系统呢？它有着什么样的特点和优势？您可以尝试与我分享更多关于文心系统或者相关方面的信息，我很乐意尝试帮助您解答关于文心的各种问题。'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"文心\", config={\"callbacks\":[handler]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab4c61-9b5f-4861-aa74-a141f69947aa",
   "metadata": {},
   "source": [
    "### 3.3、数据集与测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058c1f87-38f9-4388-a414-5e7e2c9b3084",
   "metadata": {},
   "source": [
    "1. 创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef67abc0-09d9-4548-92e8-158ed23f77c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "qa_pairs = []\n",
    "with open('example_dataset.jsonl','r',encoding='utf-8') as fp:\n",
    "    for line in fp:\n",
    "        example = json.loads(line.strip())\n",
    "        qa_pairs.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "156368ef-50aa-449f-922b-5a7ffd73f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "from langfuse.model import CreateDatasetRequest, CreateDatasetItemRequest\n",
    " \n",
    "# init\n",
    "langfuse = Langfuse()\n",
    "\n",
    "langfuse.create_dataset(CreateDatasetRequest(name=\"wiki_qa-20\"));\n",
    "\n",
    "for item in qa_pairs[:20]:\n",
    "  langfuse.create_dataset_item(\n",
    "    CreateDatasetItemRequest(\n",
    "        dataset_name=\"wiki_qa-20\",\n",
    "        # any python object or value\n",
    "        input=item[\"question\"],\n",
    "        # any python object or value, optional\n",
    "        expected_output=item[\"answer\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d43ff-fef4-4de8-bc37-ae01911561e0",
   "metadata": {},
   "source": [
    "2. 定义评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6155790-14c6-4931-b8c1-1c8de93538f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import re\n",
    "\n",
    "def bleu_score(output, expected_output):\n",
    "    def _tokenize(sentence):\n",
    "        # 正则表达式定义了要去除的标点符号\n",
    "        return re.sub(r'[^\\w\\s]', '', sentence.lower()).split()\n",
    "    \n",
    "    return sentence_bleu(\n",
    "        [_tokenize(expected_output)], \n",
    "        _tokenize(output), \n",
    "        smoothing_function=SmoothingFunction().method3\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c38fc-12d6-4696-b7cc-5f6a0f4dcae3",
   "metadata": {},
   "source": [
    "3. 定义 Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79693294-b473-43c0-84ab-424f40531997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import WikipediaRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Answer user's question according to the context below. \n",
    "Be brief, answer in no more than 20 words.\n",
    "CONTEXT_START\n",
    "{context}\n",
    "CONTEXT_END\n",
    "\n",
    "USER QUESTION:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 定义语言模型\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo-16k\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# 定义Prompt模板\n",
    "prompt = PromptTemplate.from_template(\n",
    "    prompt_template\n",
    ")\n",
    "\n",
    "# 检索 wikipedia\n",
    "retriever = WikipediaRetriever(top_k_results=1)\n",
    "\n",
    "\n",
    "# 定义输出解析器\n",
    "parser = StrOutputParser()\n",
    "\n",
    "wiki_qa_chain = (\n",
    "    {\n",
    "        \"context\": retriever, \n",
    "        \"input\": RunnablePassthrough()\n",
    "    } \n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf0a472-5af3-4904-85d5-ad66c5e9a7f2",
   "metadata": {},
   "source": [
    "4. 运行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad90f27-5aab-423a-96e8-1dc73a903cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]ERROR:langchain.callbacks.tracers.langchain:Authentication failed for https://api.smith.langchain.com/runs/64d5422b-c40f-4c93-8b8b-a98ed0a9e266. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/64d5422b-c40f-4c93-8b8b-a98ed0a9e266', '{\"detail\":\"Invalid auth\"}')\n",
      "ERROR:langchain.callbacks.tracers.langchain:Authentication failed for https://api.smith.langchain.com/runs. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs', '{\"detail\":\"Invalid auth\"}')\n",
      "100%|██████████| 20/20 [01:06<00:00,  3.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from langfuse.client import CreateScore\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = langfuse.get_dataset(\"wiki_qa-20\")\n",
    "\n",
    "for item in tqdm(dataset.items):\n",
    "    handler = item.get_langchain_handler(run_name=\"test_wiki_qa-20\")\n",
    "\n",
    "    output = wiki_qa_chain.invoke(item.input, config={\"callbacks\":[handler]})\n",
    "    \n",
    "    handler.rootSpan.score(CreateScore(\n",
    "      name=\"bleu_score\",\n",
    "      value=bleu_score(output, item.expected_output)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0559b9-2baa-4508-a9a1-c069f7714fc0",
   "metadata": {},
   "source": [
    "### 3.4、基于 LLM 的测试方法\n",
    "\n",
    "LangFuse 集成了一些原生的基于 LLM 的自动测试标准。\n",
    "\n",
    "具体参考：https://langfuse.com/docs/scores/model-based-evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b8f664-41a2-41f2-8b00-73bd0abf27ef",
   "metadata": {},
   "source": [
    "## 4、Prompt Flow\n",
    "\n",
    "项目地址 https://github.com/microsoft/promptflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f43012-06b6-42d7-a7c5-766f90bf8840",
   "metadata": {},
   "source": [
    "### 4.1、安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb12a0-3f01-49bf-aaac-d86a93beb665",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install promptflow promptflow-tools\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a8ae3f-b928-4f78-9e25-47033a2b63fd",
   "metadata": {},
   "source": [
    "### 4.2、命令行运行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7edaba2-12f6-4414-8d32-b98493a8793e",
   "metadata": {},
   "source": [
    "```sh\n",
    "pf flow init --flow ./my_chatbot --type chat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22b652-e67d-41bb-bc7c-0bdb94e475e4",
   "metadata": {},
   "source": [
    "### 4.3、VSCode 插件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c5ce9-292c-43b0-ab04-81ba5d0eae68",
   "metadata": {},
   "source": [
    "https://marketplace.visualstudio.com/items?itemName=prompt-flow.prompt-flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85434c-d9b1-4e68-9050-690db2589a9a",
   "metadata": {},
   "source": [
    "<img src=\"vsc.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e1f7b8-e7da-4c80-887c-44d4ad89478f",
   "metadata": {},
   "source": [
    "### 4.4、与 Semantic Kernel 结合使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151cf220-934e-4e11-8fc0-bea8604a38b0",
   "metadata": {},
   "source": [
    "<演示>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58790105-fafd-4a07-afab-7ff72fb0c4af",
   "metadata": {},
   "source": [
    "Azure云服务：https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/get-started-prompt-flow?view=azureml-api-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e365c65-9d5a-4fba-8d9d-b1be247f90c2",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881722dc-400a-451a-abbf-4324a0a5c2db",
   "metadata": {},
   "source": [
    "管理一个 LLM 应用的全生命周期，需要用到以下工具：\n",
    "\n",
    "1. 调试 Prompt 的 Playground\n",
    "2. 测试/验证系统的相关指标\n",
    "3. 数据集管理\n",
    "4. 各种指标监控与统计：访问量、响应时长、Token费等等\n",
    "\n",
    "根据自己的技术栈，选择：\n",
    "\n",
    "1. LangSmith: LangChain 的原始管理平台\n",
    "2. LangFuse：开源平台，支持 LangChain 和原生 OpenAI API\n",
    "3. Prompt Flow：开源平台，支持 Semantic Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd761c5-485c-4f1c-993c-990e13a1aca6",
   "metadata": {},
   "source": [
    "## 作业\n",
    "\n",
    "选择一个工具平台，对自己之前开发的系统或模型做批量测试"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
